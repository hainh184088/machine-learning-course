{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: KNN & K-Means\n",
    "Again, fill the ellipses `...` with code, and don't remove `assert` lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the Iris dataset again.\n",
    "Just goes to show that `sklearn` makes things way too easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, Y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data into training and testing set with 90:10 ratio\n",
    "# use a fixed random state for reproducible results\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score normalization.\n",
    "# Remember to scale the training and test set separately to avoid data snooping.\n",
    "# We use the training set's mu and sigma for the test set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: $k$-Nearest Neighbors\n",
    "Evaluate the test set with data from the training set.\n",
    "\n",
    "In case of ties, pick the smallest class (i.e. we prefer class 0 to class 1 to class 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "      \n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "  \n",
    "    return num \n",
    "\n",
    "\n",
    "# Remember, no training is needed for KNN!\n",
    "def evaluateKNN_single(k, x_train, y_train, data):\n",
    "    '''\n",
    "    Evaluate the classification for `data` with k-nearest neighbor\n",
    "    given training set (x_train, y_train).\n",
    "\n",
    "    Note that this function takes in one input instead of the whole\n",
    "    testing set.\n",
    "    \n",
    "    Input:\n",
    "        k      : hyperparameter for KNN\n",
    "        x_train: features of training set\n",
    "        y_train: labels of training set\n",
    "        data   : features of the data point to be evaluated\n",
    "    Output:\n",
    "        Classification of the input data point.\n",
    "    '''\n",
    "    dist = []\n",
    "    y_label = []\n",
    "    for i in range(0,y_train.shape[0]):\n",
    "        dist.append(x_train[i] - data)\n",
    "        y_label.append(y_train[i])\n",
    "\n",
    "    dist = np.array(dist)\n",
    "    distance =[]\n",
    "    for i in range(dist.shape[0]):\n",
    "        distance.append(np.sqrt(np.sum(np.square(dist[i]))))\n",
    "    # print(distance)\n",
    "    z = sorted(zip(y_label,distance), key = lambda x: x[1])\n",
    "    z= z[:k]\n",
    "    most_frequent_label,most_frequent_distance = zip(*z)\n",
    "    most_frequent_label = list(most_frequent_label)\n",
    "    return(most_frequent(most_frequent_label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code for the whole dataset\n",
    "def evaluateKNN(k, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    correct = sum(map(lambda x: evaluateKNN_single(k, x_train, y_train, x[0]) == x[1], zip(x_test, y_test)))\n",
    "    print(f'Test accuracy with k={k}: {correct/len(y_test)*100:.4f}% ({correct}/{len(y_test)})')\n",
    "    # return the number of correct evaluations for us to check with the solution\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with k=5: 100.0000% (15/15)\n"
     ]
    }
   ],
   "source": [
    "# and let's see how good is our model with k=5\n",
    "assert evaluateKNN(5) == len(y_test), \"Incorrect accuracy for 5-NN!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with k=1: 93.3333% (14/15)\n"
     ]
    }
   ],
   "source": [
    "# What if we use 1-NN?\n",
    "assert evaluateKNN(1) == len(y_test) - 1, \"Incorrect accuracy for 1-NN!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Use the first 3 data points as initial cluster centroids (medoids anyone?)\n",
    "\n",
    "Run the recaliberation step 10 times. Yes, it converges that quickly for a NP-hard problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_classification(x_data, centroids):\n",
    "    '''\n",
    "    A helper function that you will need later.\n",
    "    Classifies the points to their nearest cluster.\n",
    "    \n",
    "    Input:\n",
    "        x_data   : the data points\n",
    "        centroids: the cluster centroids\n",
    "    Output:\n",
    "        The centroid numbers that each data point belongs to (i.e. is nearest)\n",
    "    '''\n",
    "    dist = []\n",
    "    centroid_index = []\n",
    "    centroid_index = list(centroid_index)\n",
    "    for i in range(centroids.shape[0]):\n",
    "        dist.append(np.sqrt(np.sum(np.square(np.array(x_data - centroids[i])))))\n",
    "        centroid_index.append(i)\n",
    "    z = sorted(zip(centroid_index,dist), key = lambda x: x[1])\n",
    "    centroid_index, dist = zip(*z)\n",
    "    return centroid_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x_train, k, step):\n",
    "    '''\n",
    "    An implementation of K-means clustering.\n",
    "    \n",
    "    Input:\n",
    "        k      : number of clusters\n",
    "        x_train: training dataset\n",
    "        step   : number of recaliberation steps\n",
    "    Output:\n",
    "        The centroids of the clusters (a k x d matrix)\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    # create centroids\n",
    "    centroids = [x_train[0],x_train[1],x_train[2]]\n",
    "    centroids = np.array(centroids)\n",
    "    l = 0\n",
    "    # for each step, find label for each datapoint\n",
    "    while l < step:\n",
    "        label = []\n",
    "        label = list(label)\n",
    "        for i in range(x_train.shape[0]):\n",
    "            label.append(get_cluster_classification(x_train[i],centroids))\n",
    "        # for datapoints in the same label, find the mean of all datapoints and relocate the centroid\n",
    "        for i in range(k):\n",
    "            x_cluster = []\n",
    "            for j in range(x_train.shape[0]):\n",
    "                if label[j] == i:\n",
    "                    x_cluster.append(x_train[j])\n",
    "            x_cluster = np.array(x_cluster)\n",
    "            centroids[i] = np.mean(x_cluster, axis = 0)\n",
    "        l = l + 1\n",
    "    return centroids\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.56195149  0.55613693  0.51790028  0.51008192]\n",
      " [ 0.80278784 -0.13265651  1.14418849  1.30158834]\n",
      " [-0.28097574 -1.28064558  0.06241795 -0.14950677]]\n"
     ]
    }
   ],
   "source": [
    "# we know that there are three classes\n",
    "centroids = kmeans(x_train, k=3, step=10)\n",
    "# print(centroids)\n",
    "assert np.allclose(centroids, np.array([\n",
    "    [-1.02028733,  0.90854287, -1.32521428, -1.27540932],\n",
    "    [ 0.99363929,  0.01896468,  0.90355632,  0.92076921],\n",
    "    [-0.22539812, -1.02749927,  0.23322382,  0.15491878]\n",
    "])), \"Incorrect centroids for K-means!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means++\n",
    "Use the 4th data point as the intial centroid each step ([chosen with randomness](https://xkcd.com/221/)):\n",
    "- The first initial centroid should be the 4th data point.\n",
    "- The next initial centroids should be the 4th furthest data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(x_train, k, step):\n",
    "    '''\n",
    "    An implementation of K-means++ clustering.\n",
    "    \n",
    "    Input:\n",
    "        k      : number of clusters\n",
    "        x_train: training dataset\n",
    "        step   : number of recaliberation steps\n",
    "    Output:\n",
    "        The centroids of the clusters (a k x d matrix)\n",
    "    '''\n",
    "    # initialize the centroids according to the above criteria\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    l = 0\n",
    "    centroids = []\n",
    "    centroids.append(x_train[k-1])\n",
    "    current_index = []\n",
    "    current_index.append(k-1)\n",
    "    \n",
    "    # we only need to find 2 more centroids, so we need to reduce 1 step \n",
    "    while l < (k - 1):\n",
    "        # calculate the distance from the 1st centroid to other datapoints\n",
    "        dist = []\n",
    "        index = []\n",
    "        for i in range(x_train.shape[0]):\n",
    "            if i not in current_index:\n",
    "                dist.append(np.sqrt(np.sum(np.square(np.array(x_train[i] - centroids[l])))))\n",
    "                index.append(i)\n",
    "        z = sorted(zip(index,dist), key = lambda x: x[1])\n",
    "        sorted_index, sorted_dist = zip(*z)\n",
    "        centroids.append(x_train[sorted_index[-k+1]])\n",
    "        current_index.append(sorted_index[-k+1])\n",
    "        l = l + 1\n",
    "    centroids = np.array(centroids)\n",
    "    # the rest should be identical to kmeans()\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    l = 0\n",
    "    # for each step, find label for each datapoint\n",
    "    while l < step:\n",
    "        label = []\n",
    "        label = list(label)\n",
    "        for i in range(x_train.shape[0]):\n",
    "            label.append(get_cluster_classification(x_train[i],centroids))\n",
    "        # for datapoints in the same label, find the mean of all datapoints and relocate the centroid\n",
    "        for i in range(k):\n",
    "            x_cluster = []\n",
    "            for j in range(x_train.shape[0]):\n",
    "                if label[j] == i:\n",
    "                    x_cluster.append(x_train[j])\n",
    "            x_cluster = np.array(x_cluster)\n",
    "            centroids[i] = np.mean(x_cluster, axis = 0)\n",
    "        l = l + 1\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check if you did it correctly.\n",
    "centroidspp = kmeanspp(x_train, k=3, step=10)\n",
    "assert np.allclose(centroidspp, np.array([\n",
    "    [-0.0118057 , -0.87997489,  0.36942197,  0.30573876],\n",
    "    [ 1.15200055,  0.18878042,  0.98903982,  1.01136932],\n",
    "    [-1.03358934,  0.84835232, -1.32732076, -1.27380566]\n",
    "])), \"Incorrect centroids for K-means++!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Classification using clustering\n",
    "We can treat each cluster to be of a different class, and the class with most points in each cluster is the classification for that cluster. Think voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the associated classification for each cluster\n",
    "def get_cluster_label(centroids, x_train, y_train):\n",
    "    '''\n",
    "    Get the classification for each cluster using training set.\n",
    "    \n",
    "    Input:\n",
    "        centroids: the centroids of the clusters\n",
    "        x_train  : features of training set\n",
    "        y_train  : labels of training set\n",
    "    Output:\n",
    "        The classifications for the clusters.\n",
    "    '''\n",
    "    # remember to return a numpy array instead of a Python list!\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_cluster_label(centroids, x_train, y_train)\n",
    "labelspp = get_cluster_label(centroidspp, x_train, y_train)\n",
    "# each cluster nicely belongs to a different class\n",
    "assert (labels == [0, 2, 1]).all(), \"Incorrect K-means cluster label(s)!\"\n",
    "assert (labelspp == [1, 2, 0]).all(), \"Incorrect K-means++ cluster label(s)!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans_classification(centroids, labels, x_data):\n",
    "    '''\n",
    "    Get the classification for each data point using centroid labels.\n",
    "    \n",
    "    Input:\n",
    "        centroids: the centroids of the clusters\n",
    "        labels   : the labels for the clusters\n",
    "        x_data   : the data to be classified\n",
    "    Output:\n",
    "        The classifications for the data.\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the classifications\n",
    "y_train_pred = evaluate_kmeans_classification(centroids, labels, x_train)\n",
    "y_test_pred = evaluate_kmeans_classification(centroids, labels, x_test)\n",
    "y_train_pred_pp = evaluate_kmeans_classification(centroidspp, labelspp, x_train)\n",
    "y_test_pred_pp = evaluate_kmeans_classification(centroidspp, labelspp, x_test)\n",
    "\n",
    "# and check for correctness\n",
    "assert (y_train_pred == [2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 1, 2, 0, 1, 1, 0, 1, 2]).all()\n",
    "assert (y_test_pred == [1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0]).all()\n",
    "assert (y_train_pred_pp == [2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2]).all()\n",
    "assert (y_test_pred_pp == [1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate prediction accuracy\n",
    "print('[+] For K-means:')\n",
    "train_score = np.sum(y_train_pred == y_train)\n",
    "print(f'Training accuracy: {train_score / len(y_train) * 100:.4f}% ({train_score}/{len(y_train)})')\n",
    "train_score = np.sum(y_test_pred == y_test)\n",
    "print(f'Training accuracy: {train_score / len(y_test) * 100:.4f}% ({train_score}/{len(y_test)})')\n",
    "print('[+] For K-means++:')\n",
    "train_score = np.sum(y_train_pred_pp == y_train)\n",
    "print(f'Training accuracy: {train_score / len(y_train) * 100:.4f}% ({train_score}/{len(y_train)})')\n",
    "train_score = np.sum(y_test_pred_pp == y_test)\n",
    "print(f'Training accuracy: {train_score / len(y_test) * 100:.4f}% ({train_score}/{len(y_test)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fig = plt.subplot(2, 2, 1)\n",
    "sn.heatmap(confusion_matrix(y_train, y_train_pred), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means, train set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 2)\n",
    "sn.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means, test set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 3)\n",
    "sn.heatmap(confusion_matrix(y_train, y_train_pred_pp), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means++, train set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 4)\n",
    "sn.heatmap(confusion_matrix(y_test, y_test_pred_pp), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means++, test set');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
